{
  "hash": "e5e89855027f7923c47034d7b87541a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: AIC calculations\ndate: 2023-11-01\nimage: /img/aic_lm.png\ncategories:\n  - AIC\ndescription: The AIC returned by `TSLM()` is different from that returned by `lm()`. Why?\n---\n\n\nI get this question a lot, so I thought it might help to explain some issues with AIC calculation.\n\nFirst, the equation for the AIC is given by\n$$\n\\text{AIC} = 2k - 2\\log(L),\n$$\nwhere $L$ is the likelihood of the model and $k$ is the number of parameters that are estimated (including the error variance). For a linear regression model with iid $N(0,\\sigma^2)$ errors, fitted to $n$ observations, the log-likelihood can be written as\n$$\n\\log(L) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\hat{e}_i^2\n$$\nwhere $\\hat{e}_i$ is the residual for the $i$th observation. The AIC is then\n$$\n\\text{AIC} = 2k + n\\log(2\\pi) + n\\log(\\sigma^2)  + \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\hat{e}_i^2.\n$$\nSince we don't know $\\sigma^2$, we estimate it using the mean squared error (the maximum likelihood estimator), giving\n\\begin{align*}\n\\text{AIC} & = 2k +n\\log(2\\pi) + n\\log(\\text{MSE}) + n \\\\\n& = 2k + n\\log(\\text{MSE}) + C\n\\end{align*}\nwhere $C = n + n\\log(2\\pi)$ is a constant that depends only on the sample size and not on the model. This constant is often ignored. Thus, different software implementations can lead to different AIC values for the same model, since they may include or exclude the constant $C$.\n\nNow, let's look at what R returns in a simple case using the `lm()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nlibrary(fpp3)\ndf <- tibble(\n  time = seq(100),\n  x = rnorm(100),\n  y = x + rnorm(100)\n)\nfit <- lm(y ~ x, data = df)\nAIC(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 275.6267\n```\n\n\n:::\n:::\n\n\nWe can check how this is calculated by computing it ourselves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmse <- mean(residuals(fit)^2)\nn <- length(residuals(fit))\nk <- length(fit$coefficients) + 1\n# With constant\n2*k + n*log(mse) + n*log(2*pi) + n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 275.6267\n```\n\n\n:::\n\n```{.r .cell-code}\n# Without constant\n2*k + n*log(mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -8.161047\n```\n\n\n:::\n:::\n\n\nClearly, `AIC()` applied to the output from `lm()` is using the version with the constant.\n\nNow compare that with what we obtain using the `TSLM()` function from the fable package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf |>\n  as_tsibble(index = time) |>\n  model(TSLM(y ~ x)) |>\n  glance() |>\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -8.161047\n```\n\n\n:::\n:::\n\n\nThis is the AIC without the constant.\n\nThe situation is even more confusing with ARIMA models, and some other model classes, because some functions use approximations to the likelihood, rather than the exact likelihood.\n\nThus, AIC values can be compared across models fitted using the same functions, but not necessarily when models have been fitted using different functions.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}