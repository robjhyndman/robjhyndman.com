{
  "hash": "1e4bf17a83e71f04e59736e216ab74c5",
  "result": {
    "markdown": "---\ndate: 2020-02-07\ntitle: \"Electricity demand data in tsibble format\"\nslug: electrictsibbles\ncategories:\n  - time series\n  - statistics\n  - R\n  - tidyverts\n---\n\n\n\n\nThe `tsibbledata` packages contains the `vic_elec` data set, containing half-hourly electricity demand for the state of Victoria, along with corresponding temperatures from the capital city, Melbourne. These data cover the period 2012-2014.\n\nOther similar data sets are also available, and these may be of interest to researchers in the area.\n\nFor people new to tsibbles, please read my [introductory post](https://robjhyndman.com/hyndsight/tsibbles/).\n\n&nbsp;\n\n## Australian state-level demand\n\nThe rawdata for other states are also stored in the [`tsibbledata` github repository](https://github.com/tidyverts/tsibbledata/) (under the data-raw folder), but these are not included in the package to satisfy CRAN space constraints. However, anyone can still load and use the data with the following code.\n\n\n::: {.cell hash='index_cache/html/loadpackages_b5a49e33c052a447a6ce2101ca840ffc'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tsibble)\n```\n:::\n\n::: {.cell hash='index_cache/html/ausdata_fdd94f90640916e5a1292b95bb494aec'}\n\n```{.r .cell-code}\nrepo <- \"https://raw.githubusercontent.com/tidyverts/tsibbledata/master/data-raw/vic_elec/\"\nstates <- c(\"NSW\",\"QLD\",\"SA\",\"TAS\",\"VIC\")\ndirs <- paste0(repo, states, \"2015\")\n\n# Read holidays data\nholidays <- paste0(dirs,\"/holidays.txt\") %>%\n  as.list() %>%\n  map_dfr(read_csv, col_names=FALSE, .id=\"State\") %>%\n  transmute(\n    State = states[as.numeric(State)],\n    Date = dmy(X1), \n    Holiday = TRUE\n  )\n# Read temperature data\ntemperatures <- paste0(dirs,\"/temperature.csv\") %>%\n  as.list() %>%\n  map_dfr(read_csv, .id = \"State\") %>%\n  mutate(\n    State = states[as.numeric(State)],\n    Date = as_date(Date, origin = ymd(\"1899-12-30\"))\n  )\n# Read demand data\ndemands <- paste0(dirs,\"/demand.csv\") %>%\n  as.list() %>%\n  map_dfr(read_csv, .id = \"State\") %>%\n  mutate(\n    State = states[as.numeric(State)],\n    Date = as_date(Date, origin = ymd(\"1899-12-30\"))\n  )\n# Join demand, temperatures and holidays\naus_elec <- demands %>%\n  left_join(temperatures, by = c(\"State\", \"Date\", \"Period\")) %>%\n  transmute(\n    State,\n    Time = as.POSIXct(Date + minutes((Period-1) * 30)),\n    Period,\n    Date = as_date(Time),\n    DOW = wday(Date, label=TRUE),\n    Demand = OperationalLessIndustrial, \n    Temperature = Temp,\n  ) %>%\n  left_join(holidays, by = c(\"State\", \"Date\")) %>%\n  replace_na(list(Holiday = FALSE))\n# Remove duplicates and create a tsibble\naus_elec <- aus_elec %>%\n  filter(!are_duplicated(aus_elec, index=Time, key=State)) %>%\n  as_tsibble(index = Time, key=State)\n```\n:::\n\n\nThis block of code reads in raw data files containing holiday information, temperatures and electricity demand for each state, and then joins them into a single tsibble. For some reason, there are duplicated rows from South Australia, so the last few lines removes the duplicates before forming a tsibble, keyed by State.\n\n\n::: {.cell hash='index_cache/html/ausdata2_59df58cdbc350ed862574e5f4e8f457e'}\n\n```{.r .cell-code}\naus_elec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tsibble: 1,155,408 x 8 [30m] <UTC>\n# Key:       State [5]\n   State Time                Period Date       DOW   Demand Tempe…¹ Holiday\n   <chr> <dttm>               <dbl> <date>     <ord>  <dbl>   <dbl> <lgl>  \n 1 NSW   2002-01-01 00:00:00      1 2002-01-01 Tue    5714.    26.3 TRUE   \n 2 NSW   2002-01-01 00:30:00      2 2002-01-01 Tue    5360.    26.3 TRUE   \n 3 NSW   2002-01-01 01:00:00      3 2002-01-01 Tue    5015.    26.3 TRUE   \n 4 NSW   2002-01-01 01:30:00      4 2002-01-01 Tue    4603.    26.3 TRUE   \n 5 NSW   2002-01-01 02:00:00      5 2002-01-01 Tue    4285.    26.3 TRUE   \n 6 NSW   2002-01-01 02:30:00      6 2002-01-01 Tue    4075.    26.3 TRUE   \n 7 NSW   2002-01-01 03:00:00      7 2002-01-01 Tue    3943.    26.3 TRUE   \n 8 NSW   2002-01-01 03:30:00      8 2002-01-01 Tue    3884.    26.3 TRUE   \n 9 NSW   2002-01-01 04:00:00      9 2002-01-01 Tue    3878.    26.3 TRUE   \n10 NSW   2002-01-01 04:30:00     10 2002-01-01 Tue    3838.    26.3 TRUE   \n# … with 1,155,398 more rows, and abbreviated variable name ¹​Temperature\n```\n:::\n:::\n\n\nThis data set contains half-hourly data from all states from 1 January 2002 - 1 March 2015 (and in the case of Queensland to 1 April 2015). The temperature variable is from a weather station in the capital city of each state.\n\n&nbsp;\n\n## GEFCOM 2017\n\nThe [Global Energy Forecasting Competition in 2017](http://www.drhongtao.com/gefcom/2017) involved data on hourly zonal loads of ISO New England from March 2003 to April 2017. The data have already been packaged into tibble format by Cameron Roach in the [gefcom2017data Github repository](https://github.com/camroach87/gefcom2017data). So it is relatively easy to convert this to a tsibble.\n\n\n::: {.cell hash='index_cache/html/gefcom2017_f89c243f0f64227909b3800700a19e5d'}\n\n```{.r .cell-code}\n#devtools::install_github(\"camroach87/gefcom2017data\")\nlibrary(gefcom2017data)\ngefcom2017 <- gefcom %>% \n  ungroup() %>%\n  as_tsibble(key=zone, index=ts)\ngefcom2017\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tsibble: 1,241,710 x 15 [1h] <UTC>\n# Key:       zone [10]\n   ts                  zone  demand drybulb dewpnt date        year month\n   <dttm>              <chr>  <dbl>   <dbl>  <dbl> <date>     <dbl> <fct>\n 1 2003-03-01 00:00:00 CT      3386      25     19 2003-03-01  2003 Mar  \n 2 2003-03-01 01:00:00 CT      3258      23     18 2003-03-01  2003 Mar  \n 3 2003-03-01 02:00:00 CT      3189      22     18 2003-03-01  2003 Mar  \n 4 2003-03-01 03:00:00 CT      3157      22     19 2003-03-01  2003 Mar  \n 5 2003-03-01 04:00:00 CT      3166      23     19 2003-03-01  2003 Mar  \n 6 2003-03-01 05:00:00 CT      3255      23     20 2003-03-01  2003 Mar  \n 7 2003-03-01 06:00:00 CT      3430      24     20 2003-03-01  2003 Mar  \n 8 2003-03-01 07:00:00 CT      3684      24     20 2003-03-01  2003 Mar  \n 9 2003-03-01 08:00:00 CT      3977      25     21 2003-03-01  2003 Mar  \n10 2003-03-01 09:00:00 CT      4129      27     22 2003-03-01  2003 Mar  \n# … with 1,241,700 more rows, and 7 more variables: hour <dbl>,\n#   day_of_week <fct>, day_of_year <dbl>, weekend <lgl>,\n#   holiday_name <chr>, holiday <lgl>, trend <dbl>\n```\n:::\n:::\n\n\nDetails of the data (and the competition) are available on [Tao Hong's website](http://blog.drhongtao.com/2016/10/gefcom2017-hierarchical-probabilistic-load-forecasting.html).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}