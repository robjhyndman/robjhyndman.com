{
  "hash": "508b4e2c416bb0779934c529c8c3128e",
  "result": {
    "markdown": "---\ntitle: Degrees of freedom for a Ljung-Box test\ndate: 2023-06-28\ncategories:\n  - forecasting\n  - time series\nimage: /img/ETSMNN.png\nbibliography: ljung_box.bib\ndescription: The Ljung-Box test is widely used to test for autocorrelation remaining in the residuals after fitting a model to a time series. In this post, I look at the degrees of freedom used in such tests.\ncode-fold: true\nfig-height: 3\nfig-width: 8\nexecute:\n  cache: true\n---\n\n\n## The Ljung-Box test\n\nSuppose an ARMA($p,q$) model is fitted to a time series of length $T$, giving a series of residuals $e_1,\\dots,e_T$, and let the autocorrelations of this residual series be denoted by\n$$\nr_k = \\sum_{t=k+1}^T e_te_{t-k} \\Big/ \\sum_{t=1}^T e_t^2, \\qquad k=1,2,\\dots\n$$\nThe first $\\ell$ autocorrelations are used to construct the statistic\n$$\n  Q = T(T+2) \\sum_{k=1}^\\ell (T-k)^{-1}r_k^2.\n$$\n\nThis statistic was discussed by @BP70, who argued that if $T$ is large, and the model parameters correspond to the true data generating process, then $Q$ has a $\\chi^2$ distribution with $\\ell$ degrees of freedom. Later, @LB78 showed that if the model is correct, but with unknown parameters, then $Q$ has a $\\chi^2$ distribution with $\\ell-p-q$ degrees of freedom.\n\n## Extensions to other models\n\nThese days, the Ljung-Box test is applied to a lot more models than non-seasonal ARMA models, and it is not clear what the degrees of freedom should be for other models. For example:\n\n* What if the model includes an intercept term? Should that be included in the degrees of freedom calculation?\n* What about a seasonal ARIMA model? Do we just count all coefficients?\n* Or a regression with ARMA errors? Should we include the regression coefficients when computing the degrees of freedom?\n* Or an ETS model? Do we count just the smoothing parameters, or do we include the states as well, or something else?\n\nNot long ago, I had naively assumed that the correct degrees of freedom would be $\\ell-K$ where $K$ is the number of parameters estimated. I am in good company because Andrew Harvey in @harvey90 [p259] made exactly the same conjecture. That was what was coded in the [`forecast::checkresiduals()`](https://pkg.robjhyndman.com/forecast/reference/checkresiduals.html) function prior to v8.21, and how the test was applied in @fpp2 and @fpp3 until February 2023. But a recent [github discussion with Achim Zeilis](https://github.com/robjhyndman/forecast/issues/908) convinced me that it is incorrect.\n\nLet's look at a few examples. For each model, we will simulate 5000 series, each of length 250 observations. For each series, we compute the p-value of a Ljung-Box test with $\\ell=10$ and $\\ell-K$ degrees of freedom, for different values of $K$. Under the null hypothesis of uncorrelated residuals, the $p$ values should have a uniform distribution.\n\n\n::: {.cell hash='ljung_box_df_cache/html/setup_73aa324bb41a8896d092ef7907d377c6'}\n\n```{.r .cell-code}\nlibrary(forecast)\nlibrary(ggplot2)\nset.seed(0)\n\n# Function to simulate p-values given a DGP model and\n# a function to fit the model to a time series\nsimulate_pvalue <- function(model, fit_fn, l=10) {\n  ## simulate series\n  if(is.null(model$xreg)) {\n    y <- simulate(model, n = 250)\n  } else {\n    y <- simulate(model, xreg=model$xreg, n=250)\n  }\n  if(inherits(model, \"ets\")) {\n    # If multiplicative errors, fix non-positive values\n    if(model$components[1] == \"M\")\n      y[y <= 0] <- 1e-5\n  }\n  ## Fit model\n  m <- fit_fn(y)\n  ## compute p-values for various df\n  pv <- purrr::map_vec(0:3, m=m,\n    function(x, m) {\n      Box.test(residuals(m), lag = l, fitdf = x, type = \"Ljung-Box\")$p.value\n    }\n  )\n  names(pv) <- paste(\"K =\", 0:3)\n  return(pv)\n}\n# Function to replicate the above function\nsimulate_pvalues <- function(model, fit_fn, nsim = 5000, l=10) {\n  purrr::map_dfr(seq(nsim), function(x) {\n    simulate_pvalue(model, fit_fn, l=l)\n  })\n}\n# Histograms of p values\nhist_pvalues <- function(pv) {\n  pv |>\n    tidyr::pivot_longer(cols = seq(NCOL(arima_pvalues))) |>\n    ggplot(aes(x = value)) +\n    geom_histogram(bins = 30, boundary = 0) +\n    facet_grid(. ~ name) +\n    labs(title = \"P value distributions\")\n}\n# A nice table of the size of the test\ntable_pvalues <- function(pv) {\n  tibble::tibble(`test size` = c(0.01, 0.05, 0.1)) %>%\n    dplyr::bind_cols(\n      purrr::map_df(pv, function(x) {ecdf(x)(.$`test size`)})\n    ) |>\n    knitr::kable()\n}\n```\n:::\n\n\n## ARIMA models with an intercept\n\nWe will simulate from an ARIMA(2,0,0) model with a non-zero intercept. For the Ljung-Box test, we will consider $0 \\le K \\le 3$. Note that $K=0$ was the original proposal by @BP70, $K=2=p+q$ counts only ARMA coefficients, and $K=3$ counts all parameters estimated in the model. The resulting distributions of the p-values are shown below.\n\n\n::: {.cell hash='ljung_box_df_cache/html/ARIMA_bf5b946ec3736ef80cfade5f8f0a5bbd'}\n\n```{.r .cell-code}\nmodel <- Arima(sqrt(lynx), order=c(2,0,0))\nfit_fn <- function(y) {\n  Arima(y, order = c(2, 0, 0), include.mean = TRUE)\n}\narima_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(arima_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/ARIMA-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(arima_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0046| 0.0072| 0.0124| 0.0220|\n|      0.05| 0.0226| 0.0354| 0.0534| 0.0818|\n|      0.10| 0.0474| 0.0678| 0.1016| 0.1514|\n:::\n:::\n\n\nClearly the one with $K=2$ is better than the alternatives. The table shows the empirical size of the test for different threshold levels. The empirical sizes are closest to the nominal sizes when $K=2=p+q$. So we shouldn't count the intercept when computing the degrees of freedom.\n\n## Seasonal ARIMA model\n\nNext, we will simulate from an ARIMA(0,1,1)(0,1,1)$_{12}$ model, often called the \"airline model\" due to its application to the Air passenger series in @BJ2016. In fact, our DGP for the simulations will be a model fitted to the `AirPassengers` data set. Again, we consider $0\\le K \\le 3$. There are two parameters to be estimated.\n\n\n::: {.cell hash='ljung_box_df_cache/html/SARIMA_4f343c5c334a772e904f79bb38dbff4b'}\n\n```{.r .cell-code}\nmodel <- Arima(log(AirPassengers), order=c(0,1,1), seasonal=c(0,1,1))\nfit_fn <- function(y) {\n  Arima(y, order = c(0,1,1), seasonal=c(0,1,1))\n}\nsarima_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(sarima_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/SARIMA-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(sarima_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0100| 0.0170| 0.0282| 0.0456|\n|      0.05| 0.0484| 0.0684| 0.1018| 0.1452|\n|      0.10| 0.0882| 0.1218| 0.1774| 0.2526|\n:::\n:::\n\n\nInteresting. Although there are two parameters here, the tests with $K=0$ and $K=1$ do better than $K=2$. I would have expected $K=p+q+P+Q$ to be the right choice, but the test with $K=2$ has empirical size about twice the nominal size.\n\nAs a guess, perhaps the seasonal parameters aren't having an effect with $\\ell=10$. We can test what happens for larger $\\ell$ by setting $\\ell=24$ (covering two years), and repeating the exercise.\n\n\n::: {.cell hash='ljung_box_df_cache/html/SARIMA2_dc188e115e90f0c6d881e8c3058e4d5f'}\n\n```{.r .cell-code}\nsarima_pvalues <- simulate_pvalues(model, fit_fn, l=24)\nhist_pvalues(sarima_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/SARIMA2-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(sarima_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0134| 0.0164| 0.0216| 0.0278|\n|      0.05| 0.0492| 0.0614| 0.0748| 0.0940|\n|      0.10| 0.0864| 0.1066| 0.1312| 0.1590|\n:::\n:::\n\n\nI was expecting $K=2$ to do best there, but not so. $K=1$ is the most uniform, and $K=0$ gives empirical sizes closest to the nominal sizes, with the results getting worse as $K$ increases. Perhaps always setting $K=p+q$ would be a sensible strategy for ARIMA models, even if they contain seasonal components. This needs some theoretical analysis.\n\n## Regression with ARMA errors\n\nWe will simulate from a linear trend model with AR(1) errors. Here, $K=1$ counts only ARMA coefficients, while $K=3$ counts all parameters estimated. The resulting distributions of the p-values are shown below.\n\n\n::: {.cell hash='ljung_box_df_cache/html/RegARIMA_cbaf48eec0adfe101d66336169d575ee'}\n\n```{.r .cell-code}\nmodel <- Arima(10 + seq(250)/10 + arima.sim(list(ar=0.7), n=250),\n               order = c(1,0,0), xreg = seq(250))\nfit_fn <- function(y) {\n  Arima(y, order = c(1,0,0), include.constant = TRUE, xreg = seq(250))\n}\nregarima_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(regarima_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/RegARIMA-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(regarima_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0072| 0.0104| 0.0176| 0.0294|\n|      0.05| 0.0310| 0.0532| 0.0782| 0.1200|\n|      0.10| 0.0696| 0.0996| 0.1478| 0.2150|\n:::\n:::\n\n\nThe test with $K=1$ looks the most uniform, with the size of the test closest to the nominal values. So only counting ARMA coefficients seems to be correct here.\n\n## Regression model\n\nNext, we will consider a linear trend model with iid errors. That is the same as the previous model, but with a simpler error structure.\n\n\n::: {.cell hash='ljung_box_df_cache/html/trend_e646b62c4f784041c6d6ca8ffbfb8c90'}\n\n```{.r .cell-code}\nmodel <- Arima(10 + seq(250)/10 + rnorm(250),\n               order = c(0,0,0), xreg = seq(250))\nfit_fn <- function(y) {\n  Arima(y, include.constant = TRUE, xreg = seq(250))\n}\ntrend_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(trend_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/trend-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(trend_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0148| 0.0214| 0.0338| 0.0554|\n|      0.05| 0.0580| 0.0844| 0.1204| 0.1746|\n|      0.10| 0.1054| 0.1478| 0.2062| 0.2816|\n:::\n:::\n\n\nThe test with $K=0$ looks best. If we think of a regression model as a RegARIMA model with ARIMA(0,0,0) errors, this is consistent with the previous results, setting $K=p+q$.\n\n## ETS(A,N,N) model\n\nNow let's try an ETS(A,N,N) model, again using 5000 series each of length 250. If we count only the smoothing parameter, $K=1$, but if we count all estimated parameters, $K=2$. The distributions of p-values are shown below.\n\n\n::: {.cell hash='ljung_box_df_cache/html/ETSANN_9f1c0d5c8fff6f356206cab28c8540d1'}\n\n```{.r .cell-code}\nmodel <- ets(fma::strikes, \"ANN\")\nfit_fn <- function(y) {\n  ets(y, model = \"ANN\", damped = FALSE)\n}\nets_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/ETSANN-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0064| 0.0112| 0.0174| 0.0316|\n|      0.05| 0.0334| 0.0522| 0.0778| 0.1196|\n|      0.10| 0.0702| 0.0960| 0.1442| 0.2098|\n:::\n:::\n\n\n$K=1$ looks about right. That makes sense as an ETS(A,N,N) model is equivalent to an ARIMA(0,1,1) model.\n\n## ETS(M,N,N) model\n\nNext, let's try an ETS(M,N,N) model, which has no ARIMA equivalent, but which has one smoothing parameter and one initial state to estimate.\n\n\n::: {.cell hash='ljung_box_df_cache/html/ETSMNN_f5812bbdc26dd90d28e49ba2533a74d9'}\n\n```{.r .cell-code}\nmodel <- ets(fma::strikes, \"MNN\")\nfit_fn <- function(y) {\n  ets(y, model = \"MNN\", damped = FALSE)\n}\nets_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/ETSMNN-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0054| 0.0084| 0.0172| 0.0308|\n|      0.05| 0.0334| 0.0548| 0.0814| 0.1274|\n|      0.10| 0.0708| 0.1062| 0.1520| 0.2136|\n:::\n:::\n\n\nAgain, $K=1$ appears to be the best.\n\n## ETS(A,A,N) model\n\nAn ETS(A,A,N) model is equivalent to an ARIMA(0,2,2) model, so I expect this one to need $K=2$.\n\n\n::: {.cell hash='ljung_box_df_cache/html/ETSAAN_ce0bcb2507c97eab13f6cc5c746220c5'}\n\n```{.r .cell-code}\nmodel <- ets(fma::strikes, \"AAN\")\nfit_fn <- function(y) {\n  ets(y, model = \"AAN\", damped = FALSE)\n}\nets_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/ETSAAN-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0034| 0.0062| 0.0106| 0.0172|\n|      0.05| 0.0180| 0.0308| 0.0514| 0.0802|\n|      0.10| 0.0430| 0.0658| 0.1008| 0.1486|\n:::\n:::\n\n\nThis time, my conjecture is correct, and $K=2$ works well.\n\n## ETS(A,A,A) model\n\nFinally, we will check a seasonal ETS model\n\n\n::: {.cell hash='ljung_box_df_cache/html/ETSAAA_1385d6238e693baceddf0a02ce3fa351'}\n\n```{.r .cell-code}\nmodel <- ets(log(AirPassengers), model=\"AAA\", damped=FALSE)\nfit_fn <- function(y) {\n  ets(y, model = \"AAA\", damped = FALSE)\n}\nets_pvalues <- simulate_pvalues(model, fit_fn)\nhist_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n![](ljung_box_df_files/figure-html/ETSAAA-1.png){width=768}\n:::\n\n```{.r .cell-code}\ntable_pvalues(ets_pvalues)\n```\n\n::: {.cell-output-display}\n| test size|  K = 0|  K = 1|  K = 2|  K = 3|\n|---------:|------:|------:|------:|------:|\n|      0.01| 0.0178| 0.0276| 0.0396| 0.0602|\n|      0.05| 0.0634| 0.0916| 0.1360| 0.1942|\n|      0.10| 0.1206| 0.1672| 0.2244| 0.3078|\n:::\n:::\n\n\nHere there are 3 smoothing parameters, and 13 initial states to estimate. So I was expecting $K=3$ to do best, but it is the worst. Instead, $K=0$ is the best. I'm not sure what to make of this result.\n\n## Conclusions\n\nBased only on this empirical evidence:\n\n* For ARIMA models, use $\\ell-p-q$ degrees of freedom.\n* For seasonal ARIMA models, it appears that $\\ell-p-q$ also gives the best results.\n* For regression with ARIMA errors, use $\\ell-p-q$ degrees of freedom.\n* For OLS regression, use $\\ell$ degrees of freedom.\n* For non-seasonal ETS models, use $K=$ the number of smoothing parameters.\n* For seasonal ETS models, use $K=0$.\n\nThe last two of these appear to be contradictory, and it is not clear why.\n\nIt seems like this might be a good project for a PhD student to explore. In particular, can these suggestions based on empirical evidence be supported theoretically?  It would also be good to explore other models such as TBATS, ARFIMA, NNETAR, etc.\n\nFor now, I might avoid teaching the Ljung-Box test, and just get students to look at the ACF plot of the residuals instead.\n\n## Other literature\n\n* @Kim2004 shows that $Q \\sim \\chi^2_\\ell$ for an AR(1) model with ARCH errors.\n* @McLeod1983 consider the equivalent test applied to autocorrelations of squared residuals, and show that $Q^* \\sim \\chi^2_\\ell$.\n* @Mahdi2016 discusses a variation on the LB test for seasonal ARIMA models considering only autocorrelations at the seasonal lags.\n* Several other portmanteau tests (i.e., based on multiple autocorrelations) are available, and perhaps we should be using them and not the older Ljung-Box test. See @Mahdi2021 for some recent developments.\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}