{
  "hash": "76b609d258ee0063d000e785aee883ed",
  "result": {
    "markdown": "---\ndate: 2017-05-25\nslug: nnetar-prediction-intervals\ntitle: Prediction intervals for NNETAR models\ncategories:\n- forecasting\n- R\n- time series\n---\n\n\nThe `nnetar` function in the [**forecast** package for R](http://github.com/robjhyndman/forecast) fits a neural network model to a time series with lagged values of the time series as inputs (and possibly some other exogenous inputs). So it is a nonlinear autogressive model, and it is not possible to analytically derive prediction intervals. Therefore we use simulation.\n\nSuppose we fit a NNETAR model to the famous Canadian `lynx` data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(forecast)\nset.seed(2015)\n(fit <- nnetar(lynx, lambda=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: lynx \nModel:  NNAR(8,4) \nCall:   nnetar(y = lynx, lambda = 0.5)\n\nAverage of 20 networks, each of which is\na 8-4-1 network with 41 weights\noptions were - linear output units \n\nsigma^2 estimated as 95.77\n```\n:::\n:::\n\n\nI've used a Box-Cox transformation with $\\lambda=0.5$ to ensure the residuals will be roughly homoscedastic.\n\nThe model can be written as\n$$\n  y_t = f(\\boldsymbol{y}_{t-1}) + \\varepsilon_t\n$$\nwhere $\\boldsymbol{y}_{t-1} = (y_{t-1},y_{t-2},\\dots,y_{t-8})'$ is a vector containing lagged values of the series, and $f$ is a neural network with 4 hidden nodes in a single layer.\n\nThe error series $\\{\\varepsilon_t\\}$ is assumed to be homoscedastic (and possibly also normally distributed).\n\nWe can simulate future sample paths of this model iteratively, by randomly generating a value for $\\varepsilon_t$, either from a normal distribution, or by resampling from the historical values. So if $\\varepsilon^*_{T+1}$ is a random draw from the distribution of errors at time $T+1$, then\n$$\n  y^*_{T+1} = f(\\boldsymbol{y}_{T}) + \\varepsilon^*_{T+1}\n$$\nis one possible draw from the forecast distribution for $y_{T+1}$. Setting\n$\\boldsymbol{y}_{T+1}^* = (y^*_{T+1}, y_{T}, \\dots, y_{T-6})'$, we can then repeat the process to get\n$$\n  y^*_{T+2} = f(\\boldsymbol{y}^*_{T+1}) + \\varepsilon^*_{T+2}.\n$$\nIn this way, we can iteratively simulate a future sample path. By repeatedly simulating sample paths, we build up knowledge of the distribution for all future values based on the fitted neural network. Here is a simulation of 9 possible future sample paths for the lynx data. Each sample path covers the next 20 years after the observed data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim <- ts(matrix(0, nrow=20, ncol=9), start=end(lynx)[1]+1)\nfor(i in seq(9))\n  sim[,i] <- simulate(fit, nsim=20)\n\nlibrary(ggplot2)\nautoplot(lynx) + forecast::autolayer(sim)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIf we do this a few hundred or thousand times, we can get a very good picture of the forecast distributions. This is how the `forecast.nnetar` function produces prediction intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfcast <- forecast(fit, PI=TRUE, h=20)\nautoplot(fcast)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nBecause it is a little slow, `PI=FALSE` is the default, so prediction intervals are not computed unless requested. The `npaths` argument in `forecast.nnetar` controls how many simulations are done (default 1000). By default, the errors are drawn from a normal distribution. The `bootstrap` argument allows the errors to be \"bootstrapped\" (i.e., randomly drawn from the historical errors).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}