{
  "hash": "cf22a8d4a591c50fcbcd39d8ffb8e22e",
  "result": {
    "markdown": "---\ndate: 2019-10-17\ntitle: \"Non-Gaussian forecasting using fable\"\nslug: fable2\ncategories:\n  - time series\n  - graphics\n  - statistics\n  - R\n  - tidyverts\n  - forecasting\nimage: index_files/figure-html/cafe-1.png\n---\n\n\n\n::: {.cell hash='index_cache/html/loadpackages_06d7aa15a3dd918750e5fe5a55dcef4b'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(feasts)\nlibrary(fable)\n```\n:::\n\n\nIn my [previous post](https://robjhyndman.com/hyndsight/fable/) about the new [**fable** package](https://fable.tidyverts.org), we saw how fable can produce forecast distributions, not just point forecasts. All my examples used Gaussian (normal) distributions, so in this post I want to show how non-Gaussian forecasting can be done.\n\nAs an example, we will use eating-out expenditure in my home state of Victoria.\n\n\n::: {.cell hash='index_cache/html/cafe_958d5eb3431cc93026cbf2f514ec5cff'}\n\n```{.r .cell-code}\nvic_cafe <- tsibbledata::aus_retail %>%\n  filter(\n    State == \"Victoria\",\n    Industry == \"Cafes, restaurants and catering services\"\n  ) %>%\n  select(Month, Turnover)\nvic_cafe %>%\n  autoplot(Turnover) + ggtitle(\"Monthly turnover of Victorian cafes\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/cafe-1.png){width=672}\n:::\n:::\n\n\n## Forecasting with transformations\n\nClearly the variance is increasing with the level of the series, so we will consider modelling a Box-Cox transformation of the data.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_7748a7fcf1570be9e25341ccce5efdd1'}\n\n```{.r .cell-code}\nvic_cafe %>% autoplot(box_cox(Turnover, lambda = 0.2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe variance now looks more homogeneous across the series, allowing us to fit an additive model. I chose the value of $\\lambda=0.2$ by eye, but you can use the `guerrero` function for an automated approach.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_47c7d2af0b7e603f70dbe4e8c88fef8d'}\n\n```{.r .cell-code}\nvic_cafe %>% features(Turnover, guerrero)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  lambda_guerrero\n            <dbl>\n1           0.173\n```\n:::\n:::\n\n\nIt suggests something slightly smaller, but I will stick with 0.2.\n\nNow to fit a model. For this post I will use ETS models\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_53b7a208b212de9da83279b8f8d2e482'}\n\n```{.r .cell-code}\nfit <- vic_cafe %>%\n  model(ets = ETS(box_cox(Turnover, 0.2)))\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A mable: 1 x 1\n           ets\n       <model>\n1 <ETS(A,A,A)>\n```\n:::\n:::\n\n\n\n\nAn ETS(A,A,A), or additive Holt-Winters model, has been selected for the transformed data. We can produce forecasts in the usual way.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_5ed6a29be0676174838388231f834b7d'}\n\n```{.r .cell-code}\nfc <- fit %>% forecast(h = \"3 years\")\nfc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A fable: 36 x 4 [1M]\n# Key:     .model [1]\n   .model    Month        Turnover .mean\n   <chr>     <mth>          <dist> <dbl>\n 1 ets    2019 Jan  t(N(13, 0.02))  608.\n 2 ets    2019 Feb t(N(13, 0.028))  563.\n 3 ets    2019 Mar t(N(13, 0.036))  629.\n 4 ets    2019 Apr t(N(13, 0.044))  615.\n 5 ets    2019 May t(N(13, 0.052))  613.\n 6 ets    2019 Jun t(N(13, 0.061))  593.\n 7 ets    2019 Jul t(N(13, 0.069))  624.\n 8 ets    2019 Aug t(N(13, 0.077))  640.\n 9 ets    2019 Sep t(N(13, 0.085))  630.\n10 ets    2019 Oct t(N(13, 0.093))  642.\n# … with 26 more rows\n```\n:::\n:::\n\n\nNote that the distributions are given as transformed normal, denoted by t(N). The point forecast (in column `Turnover`) is the mean of this distribution. The back-transformation and bias adjustment is done automatically.\n\nOne particularly clever part of the package (thanks to Mitchell O'Hara-Wild) is that you can use any transformation in the `model()` function, and the bias adjustment is computed based on a Taylor series expansion using numerical derivatives. So you will always get the approximate mean as the point forecast, even when using some exotic transformation for which you have no analytic expression for the bias.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_69d6d5a33e02658619e53072b701e44c'}\n\n```{.r .cell-code}\nfc %>% autoplot(vic_cafe)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Bootstrapped prediction intervals\n\nIn the preceding analysis, there was still a normality assumption for the residuals of the model applied to the transformed data. If you want to avoid that as well, you can use bootstrapped intervals. These are constructed from simulated future sample paths where the residuals are resampled as possible future errors.\n\nWe can simulate future sample paths using the `generate()` function.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_75621ea31567e9e205a3d17b0f4cacfc'}\n\n```{.r .cell-code}\nsim <- fit %>% generate(h = \"3 years\", times = 5, bootstrap = TRUE)\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tsibble: 180 x 5 [1M]\n# Key:       .model, .rep [5]\n   .model .rep     Month  .innov  .sim\n   <chr>  <chr>    <mth>   <dbl> <dbl>\n 1 ets    1     2019 Jan -0.145   583.\n 2 ets    1     2019 Feb  0.324   600.\n 3 ets    1     2019 Mar  0.173   679.\n 4 ets    1     2019 Apr  0.0908  669.\n 5 ets    1     2019 May  0.0546  671.\n 6 ets    1     2019 Jun -0.122   624.\n 7 ets    1     2019 Jul -0.116   644.\n 8 ets    1     2019 Aug  0.117   689.\n 9 ets    1     2019 Sep  0.165   702.\n10 ets    1     2019 Oct -0.0674  690.\n# … with 170 more rows\n```\n:::\n:::\n\n\nHere we have generated five possible sample paths for future months. The `.rep` variable provides a new key for the tsibble. The back-transformation of the sample paths is handled automatically. If we had multiple models, each would be used to generate future sample paths provided the corresponding `generate` function existed. (In the current version of **fable**, we have not yet implemented this for ARIMA models.)\n\nThe plot below shows the five sample paths along with the last few years of historical data.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_0b8534b0035fc1067c88c43f2d794444'}\n\n```{.r .cell-code}\nvic_cafe %>%\n  filter(year(Month) >= 2008) %>%\n  ggplot(aes(x = Month)) +\n  geom_line(aes(y = Turnover)) +\n  geom_line(aes(y = .sim, colour = as.factor(.rep)), data = sim) +\n  ggtitle(\"Monthly turnover of Victorian cafes\") +\n  guides(col = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `guides(<scale> = FALSE)` is deprecated. Please use\n`guides(<scale> = \"none\")` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nPrediction intervals are calculated using percentiles of the future sample paths. This is all built into the `forecast()` function so you do not need to call `generate()` directly.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_818658bf38fc2edc5c994987445c1e5e'}\n\n```{.r .cell-code}\nfc <- fit %>% forecast(h = \"3 years\", bootstrap = TRUE)\nfc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A fable: 36 x 4 [1M]\n# Key:     .model [1]\n   .model    Month        Turnover .mean\n   <chr>     <mth>          <dist> <dbl>\n 1 ets    2019 Jan t(sample[5000])  608.\n 2 ets    2019 Feb t(sample[5000])  563.\n 3 ets    2019 Mar t(sample[5000])  628.\n 4 ets    2019 Apr t(sample[5000])  615.\n 5 ets    2019 May t(sample[5000])  612.\n 6 ets    2019 Jun t(sample[5000])  593.\n 7 ets    2019 Jul t(sample[5000])  624.\n 8 ets    2019 Aug t(sample[5000])  640.\n 9 ets    2019 Sep t(sample[5000])  630.\n10 ets    2019 Oct t(sample[5000])  642.\n# … with 26 more rows\n```\n:::\n:::\n\n\nNotice that the forecast distribution is now represented as transformed simulation with 5000 sample paths. This default number can be modified in the `times` argument for `forecast()`.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_a2cbe4b7745e836b06b998b9198de1d0'}\n\n```{.r .cell-code}\nfc %>% autoplot(vic_cafe) +\n  ggtitle(\"Monthly turnover of Victorian cafes\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIn this example, the resulting forecast intervals are almost identical to those obtained when we assumed the residuals were normally distributed.\n\n## Accuracy calculations\n\nWe can check whether the bootstrapping helped by comparing the CRPS values from both models after doing a training/test set split.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_cf1796dc218f9c55cb5bab6a0b139163'}\n\n```{.r .cell-code}\ntrain <- vic_cafe %>% filter(year(Month) <= 2014)\nfit <- train %>%\n  model(ets = ETS(box_cox(Turnover, 0.2)))\nfit %>%\n  forecast(h = \"4 years\", bootstrap = FALSE) %>%\n  accuracy(vic_cafe,\n    measures = distribution_accuracy_measures\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .model .type percentile  CRPS\n  <chr>  <chr>      <dbl> <dbl>\n1 ets    Test        24.6  24.4\n```\n:::\n\n```{.r .cell-code}\nfit %>%\n  forecast(h = \"4 years\", bootstrap = TRUE) %>%\n  accuracy(vic_cafe,\n    measures = distribution_accuracy_measures\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .model .type percentile  CRPS\n  <chr>  <chr>      <dbl> <dbl>\n1 ets    Test        24.7  24.5\n```\n:::\n:::\n\n\nIn this case it makes almost no difference which of the two approaches is used, so the non-bootstrap approach is preferred because it is much faster.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}