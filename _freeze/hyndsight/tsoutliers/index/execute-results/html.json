{
  "hash": "e8c33671c5338d889160ae19bf0a4213",
  "result": {
    "markdown": "---\ntitle: \"Detecting time series outliers\"\ndate: 2021-08-28\nslug: tsoutliers\ncategories:\n- time series\n- R\n- data science\n- anomalies\nimage: index_files/figure-html/plot-1.png\n---\n\n\n\n\nThe [`tsoutliers()` function](https://pkg.robjhyndman.com/forecast/reference/tsoutliers.html) in the [*forecast* package for R](https://pkg.robjhyndman.com/forecast/) is useful for identifying anomalies in a time series. However, it is not properly documented anywhere. This post is intended to fill that gap.\n\nThe function began as [an answer on CrossValidated](https://stats.stackexchange.com/a/1153/159) and was later added to the *forecast* package because I thought it might be useful to other people. It has since been updated and made more reliable.\n\nThe procedure [decomposes the time series into trend, seasonal and remainder components](https://otexts.com/fpp2/components.html):\n$$y_t = T_t + S_t + R_t.$$\nThe seasonal component is optional, and it may containing several seasonal patterns corresponding to the seasonal periods in the data. The idea is to first remove any seasonality and trend in the data, and then find outliers in the remainder series, $R_t$.\n\nFor data observed more frequently than annually, we use a robust approach to estimate $T_t$ and $S_t$ by first applying the [MSTL method](https://robjhyndman.com/publications/mstl/) to the data. MSTL will iteratively estimate the seasonal component(s).\n\nThen the strength of seasonality is measured using\n$$F_s = 1-\\frac{\\text{Var}(y_t - \\hat{T}_t - \\hat{S}_t)}{\\text{Var}(y_t - \\hat{T}_t)}.$$\nIf $F_s>0.6$, a seasonally adjusted series is computed:\n$$y_t^* = y_t - \\hat{S}_t.$$\nA seasonal strength threshold is used here because the estimate of $\\hat{S}_t$ is likely to be overfitted and very noisy if the underlying seasonality is too weak (or non-existent), potentially masking any outliers by having them absorbed into the seasonal component.\n\nIf $F_s \\le 0.6$, or if the data is observed annually or less frequently, we simply set $y_t^*=y_t$.\n\nNext, we re-estimate the trend component from the $y_t^*$ values. For non-seasonal time series such as annual data, this is necessary as we don't have the trend estimate from the STL decomposition. But even if we *have* computed an STL decomposition, we may not have used it if $F_s \\le 0.6$.\n\nThe trend component $T_t$ is estimated by applying Friedman's super smoother (via `supsmu()`) to the $y_t^*$ data. This function has been tested on lots of data and tends to work well on a wide range of problems.\n\nWe look for outliers in the estimated remainder series\n$$\\hat{R}_t = y^*_t - \\hat{T}_t.$$\nIf $Q1$ denotes the 25th percentile and $Q3$ denotes the 75th percentile of the remainder values, then the interquartile range is defined as $IQR = Q3-Q1$. Observations are labelled as outliers if they are less than $Q1 - 3\\times IQR$ or greater than $Q3 + 3\\times IQR$. This is the definition used by [Tukey (1977, p44)](http://buy.geni.us/Proxy.ashx?TSID=140570&GR_URL=http%3A%2F%2Fwww.amazon.com%2Fdp%2F0134995457\n) in his original boxplot proposal for \"far out\" values.\n\n\n\n\n\nIf the remainder values are normally distributed, then the probability of an observation being identified as an outlier is approximately 1 in 427000.\n\nAny outliers identified in this manner are replaced with linearly interpolated values using the neighbouring observations, and the process is repeated.\n\n## Example: Gold data\n\nThe gold price data contains daily morning gold prices in US dollars from 1 January 1985 to 31 March 1989. The data was given to me by a client who wanted me to forecast the gold price. (I told him it would be almost impossible to beat a naive forecast). The data are shown below.\n\n```r\nlibrary(fpp2)\n```\n\n\n::: {.cell hash='index_cache/html/plot_bbab1a4d8772bfbb7cdd557e83330cd7'}\n\n```{.r .cell-code}\nautoplot(gold)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-1.png){width=672}\n:::\n:::\n\n\nThere are periods of missing values, and one obvious outlier which is about $100 greater than what would be expected. This was simply a typo, with someone typing 593.70 rather than 493.70. Let's see if the `tsoutliers()` function can spot it.\n\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_d7d43cd590424f8780fcdfd27dcc55db'}\n\n```{.r .cell-code}\ntsoutliers(gold)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$index\n[1] 770\n\n$replacements\n[1] 495\n```\n:::\n:::\n\n\nSure enough, it is easily found and the suggested replacement (linearly interpolated) is close to the true value.\n\nThe `tsclean()` function removes outliers identified in this way, and replaces them (and any missing values) with linearly interpolated replacements.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_640d1b9f0b79991a946461a9c8674c6a'}\n\n```{.r .cell-code}\nautoplot(tsclean(gold), series=\"clean\", color='red', lwd=0.9) +\n  autolayer(gold, series=\"original\", color='gray', lwd=1) +\n  geom_point(data = tsoutliers(gold) %>% as.data.frame(),\n             aes(x=index, y=replacements), col='blue') +\n  labs(x = \"Day\", y = \"Gold price ($US)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe blue dot shows the replacement for the outlier, the red lines show the replacements for the missing values.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}