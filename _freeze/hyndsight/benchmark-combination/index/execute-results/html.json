{
  "hash": "44d979c3f0f9968b12d1123bd7087a8f",
  "result": {
    "markdown": "---\ndate: 2018-06-24\nslug: benchmark-combination\ntitle: \"A forecast ensemble benchmark\"\ncategories:\n  - forecasting\n  - time series\n  - R\n---\n\n\n\n\n[Forecasting benchmarks](/hyndsight/benchmarks) are very important when testing new forecasting methods, to see how well they perform against some simple alternatives. Every week I get sent papers proposing new forecasting methods that fail to do better than even the simplest benchmark. They are rejected without review.\n\n[Typical benchmarks](https://otexts.org/fpp2/simple-methods.html) include the naïve method (especially for finance and economic data), the seasonal naïve method (for seasonal data), an automatically selected [ETS model](https://otexts.org/fpp2/ets.html), and an automatically selected [ARIMA model](https://otexts.org/fpp2/arima.html). These are easily calculated using the `naive()`, `snaive()`, `ets()` and `auto.arima()` functions in the forecast package. Note that the seasonal naïve method is equivalent to the naïve method for annual data (or any other data with frequency=1).\n\nAnother benchmark worth considering is the Theta method which did very well in the M3 forecasting competition. The `thetaf()` function provides a convenient implementation.\n\nAt the [International Symposium on Forecasting](http://isf.forecasters.org) held in Boulder last week, [Srihari Jaganathan](https://forecasters.org/blog/2017/01/30/member-profile-srihari-jaganathan/) proposed that a simple average of these forecasts should be used as a standard forecast combination (or ensemble) benchmark. This is a good idea as it is very easy to do, relatively fast to compute, and often provides an excellent forecast. (For other combinations, the [forecastHybrid](https://cran.r-project.org/package=forecastHybrid) package might be useful.)\n\nIn this post, I will test various subsets of the four benchmark methods mentioned above to see which performs best. Here is some R code to compute all possible subset ensembles from these four benchmarks. That gives 15 ($=2^4-1$) possible combinations.\n\nThe code is general enough that it is easy to add additional methods if anyone wants to include other possible benchmark methods. Just name them with a unique letter in the following function; no other code in this post needs to be changed.\n\n\n::: {.cell hash='index_cache/html/benchmarks_3e2e9f489a209f68c70b7e3a44a811bb'}\n\n```{.r .cell-code}\nbenchmarks <- function(y, h) {\n  require(forecast)\n  # Compute four benchmark methods\n  fcasts <- rbind(\n    N = snaive(y, h)$mean,\n    E = forecast(ets(y), h)$mean,\n    A = forecast(auto.arima(y), h)$mean,\n    T = thetaf(y, h)$mean)\n  colnames(fcasts) <- seq(h)\n  method_names <- rownames(fcasts)\n  # Compute all possible combinations\n  method_choice <- rep(list(0:1), length(method_names))\n  names(method_choice) <- method_names\n  combinations <- expand.grid(method_choice) %>% tail(-1) %>% as.matrix()\n  # Construct names for all combinations\n  for (i in seq(NROW(combinations))) {\n    rownames(combinations)[i] <- paste0(method_names[which(combinations[i, ] > 0)],\n      collapse = \"\")\n  }\n  # Compute combination weights\n  combinations <- sweep(combinations, 1, rowSums(combinations), FUN = \"/\")\n  # Compute combinations of forecasts\n  return(combinations %*% fcasts)\n}\n```\n:::\n\n\nLet's try it out on the M3 competition data.\n\n\n::: {.cell hash='index_cache/html/m3comparison_e4c76b02a3ad8aee783ef708daf49d24'}\n\n```{.r .cell-code}\nlibrary(Mcomp)\nlibrary(tidyverse)\n# Compute \"symmetric\" percentage errors and scaled errors\nerrors <- map(M3, function(u) {\n  train <- u$x\n  test <- u$xx\n  f <- benchmarks(train, u$h)\n  error <- -sweep(f, 2, test)\n  pcerror <- (200 * abs(error) / sweep(abs(f), 2, abs(test), FUN = \"+\")) %>%\n    as_tibble() %>%\n    mutate(Method = rownames(f)) %>%\n    gather(key = h, value = sAPE, -Method)\n  scalederror <- (abs(error) / mean(abs(diff(train, lag = frequency(train))))) %>%\n    as_tibble() %>%\n    mutate(Method = rownames(f)) %>%\n    gather(key = h, value = ASE, -Method)\n  return(list(pcerror = pcerror, scalederror = scalederror))\n})\n# Construct a tibble with all results\nM3errors <- tibble(\n    Series = names(M3),\n    Period = map_chr(M3, \"period\"),\n    se = map(errors, \"scalederror\"),\n    pce = map(errors, \"pcerror\")) %>%\n  unnest() %>%\n  select(-h1, -Method1) %>%\n  mutate(h = as.integer(h),\n         Period = factor(str_to_title(Period),\n                         levels = c(\"Monthly\",\"Quarterly\",\"Yearly\",\"Other\")))\n```\n:::\n\n\nWe need to average the accuracy measures for each period, horizon and method.\n\n\n::: {.cell hash='index_cache/html/accuracy_541f95356499488072870404aa11a531'}\n\n```{.r .cell-code}\naccuracy <- M3errors %>%\n  group_by(Period, Method, h) %>%\n  summarize(MASE=mean(ASE), sMAPE=mean(sAPE)) %>%\n  ungroup()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'Period', 'Method'. You can override using\nthe `.groups` argument.\n```\n:::\n:::\n\n\nTo keep the table of results of a manageable size, I have summarised the accuracy statistics over all forecast horizons, and included only the best combination approach plus the original benchmark methods. The table is ordered by MASE value with the best method at the top for each period.\n\n\n::: {.cell hash='index_cache/html/table_9c08af0d2902461895388fd075e939d9'}\n\n```{.r .cell-code}\n# Find names of original methods\noriginal_methods <- unique(accuracy[[\"Method\"]])\noriginal_methods <- original_methods[str_length(original_methods)==1L]\n# Compute summary table of accuracy measures\naccuracy_table <- accuracy %>%\n  group_by(Method,Period) %>%\n  summarise(\n    sMAPE = mean(sMAPE, na.rm = TRUE),\n    MASE = mean(MASE, na.rm = TRUE) ) %>%\n  arrange(MASE) %>% ungroup()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'Method'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nbest <- accuracy_table %>% filter(MASE==min(MASE))\naccuracy_table <- accuracy_table %>%\n  filter(Method %in% original_methods | Method %in% best$Method) %>%\n  arrange(Period, MASE) %>%\n  select(Period, Method, sMAPE, MASE)\nknitr::kable(accuracy_table)\n```\n\n::: {.cell-output-display}\n|Period    |Method | sMAPE|  MASE|\n|:---------|:------|-----:|-----:|\n|Monthly   |EAT    | 13.53| 0.828|\n|Monthly   |T      | 13.86| 0.864|\n|Monthly   |E      | 14.14| 0.865|\n|Monthly   |A      | 15.02| 0.868|\n|Monthly   |N      | 17.23| 1.146|\n|Quarterly |EAT    |  9.01| 1.064|\n|Quarterly |T      |  9.20| 1.117|\n|Quarterly |E      |  9.68| 1.170|\n|Quarterly |A      | 10.01| 1.189|\n|Quarterly |N      | 11.06| 1.425|\n|Yearly    |EAT    | 16.03| 2.688|\n|Yearly    |T      | 16.76| 2.774|\n|Yearly    |E      | 17.00| 2.860|\n|Yearly    |A      | 17.10| 2.959|\n|Yearly    |N      | 17.88| 3.172|\n|Other     |EAT    |  4.32| 1.807|\n|Other     |E      |  4.37| 1.814|\n|Other     |A      |  4.51| 1.841|\n|Other     |T      |  4.92| 2.271|\n|Other     |N      |  6.30| 3.089|\n:::\n:::\n\n\nThe best performing ensemble method is EAT -- a combination of an ETS model, an ARIMA model and the Theta method.\n\nNext, we can plot the accuracy statistics against the forecast horizon. The legend has been ordered to correspond roughly to the order of the series in the MASE plots.\n\n\n::: {.cell hash='index_cache/html/plots_fada838d0da89ed59644108d5262776d'}\n\n```{.r .cell-code}\norder <- accuracy_table %>% group_by(Method) %>%\n  summarise(MASE = mean(MASE)) %>% arrange(MASE) %>%\n  pull(\"Method\") %>% rev()\naccuracy %>%\n  gather(key = \"Measure\", value = \"accuracy\", sMAPE, MASE) %>%\n  filter(Method %in% unique(accuracy_table$Method)) %>%\n  mutate(Method = factor(Method, levels=order)) %>%\n  ggplot(aes(x = h, y = accuracy), group = Method) +\n    geom_line(aes(col = Method)) +\n    facet_grid(rows = vars(Measure), cols=vars(Period), scale = \"free\") +\n    xlab(\"Forecast horizon\") + ylab(\"Forecast accuracy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plots-1.png){width=960}\n:::\n:::\n\n\nFrom the graph we can see that the EAT ensemble works well for all periods and almost all horizons. To finish, here is a function which produces the EAT ensemble as a forecast object.\n\n\n::: {.cell hash='index_cache/html/eat_7cbd8df9eaad765115769f49192623b0'}\n\n```{.r .cell-code}\neat_ensemble <- function(y, h = ifelse(frequency(y) > 1, 2*frequency(y), 10)) {\n  require(forecast)\n  fets <- forecast(ets(y), h)\n  farima <- forecast(auto.arima(y), h)\n  ftheta <- thetaf(y, h)\n  comb <- fets\n  comb$mean <-(fets$mean + farima$mean + ftheta$mean)/3\n  comb$method <- \"ETS-ARIMA-Theta Combination\"\n  return(comb)\n}\nUSAccDeaths %>% eat_ensemble() %>% autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/eat-1.png){width=672}\n:::\n:::\n\n\nThe prediction intervals shown are simply those from the ETS model. Combining prediction intervals is more difficult, and I'll leave that to another post.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}