---
date: 2009-12-02 06:07:26+00:00
slug: replications
title: Replications and reproducible research
categories:
- reproducible research
---

### Reproducible research

One of the best ways to get started with research in a new area is to try to replicate some existing research. In doing so, you will usually gain a much better understanding of the topic, and you will often discover some problems with the research, or develop ideas that will lead to a new research paper.

Unfortunately, a lot of papers are not reproducible because the data are not made available, or the description of the methods are not detailed enough. The good news is that there is a growing move amongst funding agencies and journals to make more research reproducible.  Peng, Dominici and Zeger (2006) and Koenker and Zeileis (2009) provide helpful discussions of new tools (especially Sweave) for making research easier to reproduce.

The _International Journal of Forecasting_ is also encouraging researchers to make their data and computer code available in order to allow others to replicate the research. I have just written an [editorial](/publications/replication) on this topic which will appear in the first issue of 2010. Here is an excerpt from the article:

>As the leading journal in forecasting, the IJF has a responsibility to set research standards.

>So, a couple of years ago, we started asking authors to make their data and code available on our website. Then last year we changed our guidelines for authors to say

>>Authors will normally be expected to submit a complete set of any data used in electronic form, or provide instructions for how to obtain them. Exceptions to this requirement may be made at the discretion of the handling editor. The author must describe methods and data sufﬁciently so the research can be replicated. The
provision of code as well as data is encouraged, but not required.

>This is consistent with the moves of many granting agencies which are now starting to require publicly funded research to make the data publicly available. Once the data are public, other researchers can verify (or otherwise) the conclusions drawn.

>Six months ago, the _International Journal of Forecasting_ website ([ijf.forecasters.org](http://ijf.forecasters.org)) was redesigned to allow supplements and comments on each published paper. Supplementary information about a paper can be provided by authors and is freely available online. This can include data, computer code, large tables, extra ﬁgures, extended footnotes, extra relevant material, etc. Authors are required to provide whatever material is needed allow their results to be replicated without excessive difﬁculty.


### Replication articles

It has become standard in most sciences for results to be replicated before being widely accepted. Remember cold fusion? Research findings that cannot be independently verified under the same or very similar conditions are little more than published opinions. In fact, the painstaking step-by-step duplication of published research is often the only way to properly assess the work done by others (Laine et al, 2007). While replicating research is accepted practice in medicine, chemistry, physics, and many other areas of science, it has not been part of the research culture in statistics, econometrics and other fields associated with forecasting.

The _International Journal of Forecasting_ is trying to change this culture, and is willing to publish replication articles, especially if they provide new insights into published results. For example, we published Gardner & Diaz-Saiz (2008) which attempted to replicate Fildes et al (1998) and provided some useful new insight into the original results. In the next issue of the journal, there will also be an invited paper by Heiner Evanschitzky and Scott Armstrong on replications in forecasting research.  I hope everyone working in forecasting, statistics, econometrics and related fields will soon come to see replication studies as an important part of the research process.

### References



  1. [Evanschitzky, H. & Armstrong, J. S. (2010). Replications of forecasting research. _International Journal of Forecasting_, **26**, to appear.](http://dx.doi.org/10.1016/j.ijforecast.2009.09.003)

  2. [Fildes, R., Hibon, M., Makridakis, S., & Meade, N. (1998). Generalising about univariate forecasting methods: Further empirical evidence. _International Journal of Forecasting_, **14**, 339–358.](http://dx.doi.org/10.1016/S0169-2070(98)00009-0)

  3. [Gardner, Jr., E. S. & Diaz-Saiz, J. (2008). Exponential smoothing in the telecommunications data. _International Journal of Forecasting_, **24**, 170–174.](http://dx.doi.org/10.1016/j.ijforecast.2007.05.002)

  4. [Hyndman, R.J. (2010) Encouraging replications and reproducible research. _International Journal of Forecasting_, **26**, to appear.](/publications/replication)

  5. [Koenker, R. & Zeilis, A. (2009). On reproducible econometric research. _Journal of Applied Econometrics_, **24**(5), 833–847.](http://dx.doi.org/10.1002/jae.1083)

  6. [Laine, C., Goodman, S. N., Griswold, M. E., & Sox, H. C. (2007). Reproducible research: moving toward research the public can really trust. _Annals of Internal Medicine_, 146 (6), 450–453.](http://www.annals.org/content/146/6/450)

  7. [Peng, R. D., Dominici, F., & Zeger, S. L. (2006). Reproducible epidemiologic research. _American Journal of Epidemiology_, **163** (9), 783–789.](http://dx.doi.org/10.1093/aje/kwj093)


### Some useful websites



  * [Reproducibility in econometrics research](http://www.econ.uiuc.edu/~roger/repro.html)

  * [Reproducible Research](http://reproducibleresearch.net)

  * [Reproducible Research blog](http://www.reproducibleresearch.net/blog/)
